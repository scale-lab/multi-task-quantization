# --------------------------------------------------------
# Swin Transformer
# Copyright (c) 2021 Microsoft
# Licensed under The MIT License [see LICENSE for details]
# Written by Ze Liu
# --------------------------------------------------------

from .swin_transformer import SwinTransformer
from .swin_mlp import SwinMLP
from .swin_mult import MultiTaskSwin


def build_model(config, mixed_precision_bits, is_pretrain=False, is_teacher=False):
    model_type = config.MODEL.TYPE

    # accelerate layernorm
    if config.FUSED_LAYERNORM:
        try:
            import apex as amp
            layernorm = amp.normalization.FusedLayerNorm
        except:
            layernorm = None
            print("To use FusedLayerNorm, please install apex.")
    else:
        import torch.nn as nn
        layernorm = nn.LayerNorm


    if model_type == 'swin' or is_teacher:
        model = SwinTransformer(mixed_precision_bits[0],
                                mixed_precision_bits[1],
                                img_size=config.DATA.IMG_SIZE,
                                patch_size=config.MODEL.SWIN.PATCH_SIZE,
                                in_chans=config.MODEL.SWIN.IN_CHANS,
                                num_classes=config.MODEL.NUM_CLASSES,
                                embed_dim=config.MODEL.SWIN.EMBED_DIM,
                                depths=config.MODEL.SWIN.DEPTHS,
                                num_heads=config.MODEL.SWIN.NUM_HEADS,
                                window_size=config.MODEL.SWIN.WINDOW_SIZE,
                                mlp_ratio=config.MODEL.SWIN.MLP_RATIO,
                                qkv_bias=config.MODEL.SWIN.QKV_BIAS,
                                qk_scale=config.MODEL.SWIN.QK_SCALE,
                                drop_rate=config.MODEL.DROP_RATE,
                                drop_path_rate=config.MODEL.DROP_PATH_RATE,
                                ape=config.MODEL.SWIN.APE,
                                norm_layer=layernorm,
                                patch_norm=config.MODEL.SWIN.PATCH_NORM,
                                use_checkpoint=config.TRAIN.USE_CHECKPOINT,
                                fused_window_process=config.FUSED_WINDOW_PROCESS)
    elif model_type == 'swin_mlp':
        model = SwinMLP(img_size=config.DATA.IMG_SIZE,
                        patch_size=config.MODEL.SWIN_MLP.PATCH_SIZE,
                        in_chans=config.MODEL.SWIN_MLP.IN_CHANS,
                        num_classes=config.MODEL.NUM_CLASSES,
                        embed_dim=config.MODEL.SWIN_MLP.EMBED_DIM,
                        depths=config.MODEL.SWIN_MLP.DEPTHS,
                        num_heads=config.MODEL.SWIN_MLP.NUM_HEADS,
                        window_size=config.MODEL.SWIN_MLP.WINDOW_SIZE,
                        mlp_ratio=config.MODEL.SWIN_MLP.MLP_RATIO,
                        drop_rate=config.MODEL.DROP_RATE,
                        drop_path_rate=config.MODEL.DROP_PATH_RATE,
                        ape=config.MODEL.SWIN_MLP.APE,
                        patch_norm=config.MODEL.SWIN_MLP.PATCH_NORM,
                        use_checkpoint=config.TRAIN.USE_CHECKPOINT)
    else:
        raise NotImplementedError(f"Unkown model: {model_type}")

    return model


def build_mtl_model(backbone, config, mixed_precision_bits, is_teacher=False):
    if config.MODEL.FREEZE_BACKBONE:
        print("Freezing backbone")
        for param in backbone.parameters():
            param.requires_grad = False
    embed_dim = config.MODEL.SWIN.EMBED_DIM
    decoder_cfg = {
        'embed_dim': embed_dim,
        'decoder_dim': config.MODEL.SWIN.DECODER_DIM,
        'depths': config.MODEL.SWIN.DEPTHS,
        'dims': [2*embed_dim, 4*embed_dim, 8*embed_dim, 8*embed_dim],
        'patch_res': config.MODEL.SWIN.DECODER_PATCH_RES,
        'window_size': config.MODEL.SWIN.WINDOW_SIZE,
        'upsampling': 'deconv'
    }

    model = MultiTaskSwin(backbone, decoder_cfg, config, mixed_precision_bits, task_map_file=config.MODEL.DECODER_MAP_FILE)

    if config.TRAIN.CONTROLLERS_PRETRAIN:
        print("Freezing all but controllers")
        # model.train(mode=False)
        for p in model.parameters():
            p.requires_grad = False

        model.backbone.unfreeze_controllers()

    if is_teacher:
        model.freeze_all()
    return model
